{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63fd3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# import gluonnlp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "GPUIdx = \"2,3\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= GPUIdx\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833ca16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "Tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "Embedding = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29882890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18363a82dfc4426ba228dd6b9d94c74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30587 512\n"
     ]
    }
   ],
   "source": [
    "UNCASE = True\n",
    "\n",
    "# FileObject = open(\"../../Data/DefinitionDataset/LexicoDefinition.txt\", 'r', encoding='utf-8')\n",
    "FileObject = open(\"./data/DefinitionDatasetNoPOS\", 'r', encoding='utf-8')\n",
    "VocabDefExamDict = {}\n",
    "MAX_SEQ_LEN = 512\n",
    "pbar = tqdm(total = 1)\n",
    "for line in FileObject:\n",
    "    Contents = line.split(\" (def.) \")\n",
    "    Word = Contents[0].strip()\n",
    "#     print(Word)\n",
    "    if UNCASE: Word = Word.lower()\n",
    "    ### Word -> Definition1 -> [ex1, ex2]\n",
    "    Defs = {}\n",
    "    for c in Contents[1:]:\n",
    "        c = c.split(\" (ex.) \")\n",
    "        Def = c[0].strip(); Examples = c[1].strip()\n",
    "        if UNCASE:\n",
    "            Def = Def.lower()\n",
    "            Examples = Examples.lower()\n",
    "            \n",
    "        Examples = Examples.split(' | ')\n",
    "        ### When don't know MAX_SEQ_LEN\n",
    "#         SEQ_LEN =  max(*[len(Tokenizer.encode(e)) for e in Examples], len(Tokenizer.encode(Def)))\n",
    "#         if SEQ_LEN > MAX_SEQ_LEN:\n",
    "#             MAX_SEQ_LEN = SEQ_LEN\n",
    "        Defs[Def] = Examples if len(Examples) else ''\n",
    "    \n",
    "    if Word in VocabDefExamDict.keys():\n",
    "        VocabDefExamDict[Word].update(Defs)\n",
    "    else:\n",
    "        VocabDefExamDict[Word] = Defs\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "MAX_SEQ_LEN = min(512, MAX_SEQ_LEN)\n",
    "MAX_SEQ_LEN = max(256, MAX_SEQ_LEN)\n",
    "\n",
    "print(len(VocabDefExamDict), MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b817dd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac486753a4f402fa9b68b970432f3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb483a6b58e4e0baeb3e39e5eb43032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e85230bedab40b39db7e086b83e437e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598b4d05ae314b53833e9f541bde0952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PAIR_list = [\"Word-Word\", \"Word-Def\", \"Word-ExamWord\", \"Def-ExamWord\"]\n",
    "PAIR_cnt = dict(zip(PAIR_list, [0]*len(PAIR_list)))\n",
    "\n",
    "WordDef_train_src, DefExam_train_src, WordExam_train_src = [], [], []\n",
    "WordWord_train_src, WordExWord_train_src, DefExWord_train_src = [], [], []\n",
    "WordDef_train_tgt, DefExam_train_tgt, WordExam_train_tgt = [], [], []\n",
    "WordWord_train_tgt, WordExWord_train_tgt, DefExWord_train_tgt = [], [], []\n",
    "\n",
    "### Saved Indices\n",
    "for PAIR in PAIR_list:\n",
    "    pbar = tqdm(total=len(VocabDefExamDict))\n",
    "    for Word in VocabDefExamDict:\n",
    "        if PAIR == \"Word-Word\":\n",
    "            WordWord_train_src.append(Word+'|'+Word)\n",
    "            WordWord_train_tgt.append(Word+'|'+Word)\n",
    "            PAIR_cnt[PAIR] += 1\n",
    "        \n",
    "        elif PAIR == \"Word-Def\":\n",
    "            Defs = list(VocabDefExamDict[Word].keys())\n",
    "            for d in Defs:\n",
    "                WordDef_train_src.append(Word+'|'+Word)\n",
    "                WordDef_train_tgt.append(Word+'|'+d)\n",
    "                PAIR_cnt[PAIR] += 1\n",
    "                \n",
    "        elif PAIR == \"Word-ExamWord\":\n",
    "            w = VocabDefExamDict[Word]\n",
    "            Defs = list(w.keys())\n",
    "            for d in Defs:\n",
    "                for ex in w[d]:\n",
    "                    if ex != '':\n",
    "                        WordExWord_train_src.append(Word+'|'+Word)\n",
    "                        WordExWord_train_tgt.append(Word+'|'+ex)\n",
    "                        PAIR_cnt[PAIR] += 1\n",
    "                        \n",
    "        elif PAIR == \"Def-ExamWord\":\n",
    "            w = VocabDefExamDict[Word]\n",
    "            Defs = list(w.keys())\n",
    "            for d in Defs:\n",
    "                for ex in w[d]:\n",
    "                    if ex != '':\n",
    "                        DefExWord_train_src.append(Word+'|'+d)\n",
    "                        DefExWord_train_tgt.append(Word+'|'+ex)\n",
    "                        PAIR_cnt[PAIR] += 1\n",
    "        else:\n",
    "            print(\"Out-of-Bound\")\n",
    "            break\n",
    "        ### For Debug\n",
    "#         break\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "# WordDef_train_src, DefExam_train_src, WordExam_train_src = torch.tensor(WordDef_train_src), torch.tensor(DefExam_train_src), torch.tensor(WordExam_train_src)\n",
    "# WordWord_train_src, WordExWord_train_src, DefExWord_train_src = torch.tensor(WordWord_train_src), torch.tensor(WordExWord_train_src), torch.tensor(DefExWord_train_src)\n",
    "# WordDef_train_tgt, DefExam_train_tgt, WordExam_train_tgt = torch.tensor(WordDef_train_tgt), torch.tensor(DefExam_train_tgt), torch.tensor(WordExam_train_tgt)\n",
    "# WordWord_train_tgt, WordExWord_train_tgt, DefExWord_train_tgt = torch.tensor(WordWord_train_tgt), torch.tensor(WordExWord_train_tgt), torch.tensor(DefExWord_train_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0111b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DefinitionDataset(Dataset):\n",
    "    def __init__(self, x=None, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = len(self.x)\n",
    "        self.token_config = {\"truncation\": True, \"padding\": \"max_length\",\n",
    "                            \"max_length\": MAX_SEQ_LEN, \"return_tensors\": \"pt\"}\n",
    "    def __getitem__(self, index):\n",
    "        target_w = self.x[index].split('|')[0]\n",
    "        x = self.x[index].split('|')[1]\n",
    "        y = self.y[index].split('|')[1]\n",
    "        w_tokenized = Tokenizer(target_w, add_special_tokens=False, padding=\"max_length\",\n",
    "                                max_length=10, return_tensors=\"pt\")\n",
    "        x_tokenized = Tokenizer(x, **self.token_config)\n",
    "        y_tokenized = Tokenizer(y, **self.token_config)\n",
    "        return w_tokenized, x_tokenized, y_tokenized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0d02d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30587 93344 1168702 1168702\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "Hyperparams = {\n",
    "    \"NumEpochs\": 10,\n",
    "    \"BatchSize\": 2**7,\n",
    "#     \"BatchSize\": 2**5,\n",
    "#     \"LearningRate\": 2e-3,\n",
    "    \"LearningRate\": 1e-5,\n",
    "}\n",
    "\n",
    "train_dataset_WordWord = DefinitionDataset(x=WordWord_train_src, y=WordWord_train_tgt) if len(WordWord_train_src) else None\n",
    "train_dataset_WordDef = DefinitionDataset(x=WordDef_train_src, y=WordDef_train_tgt) if len(WordDef_train_src) else None\n",
    "train_dataset_WordExWord = DefinitionDataset(x=WordExWord_train_src, y=WordExWord_train_tgt) if len(WordExWord_train_src) else None\n",
    "train_dataset_DefExWord = DefinitionDataset(x=DefExWord_train_src, y=DefExWord_train_tgt)  if len(DefExWord_train_src) else None\n",
    "\n",
    "train_loader_WordWord = DataLoader(dataset=train_dataset_WordWord, batch_size=Hyperparams[\"BatchSize\"], shuffle=True, num_workers=0)\n",
    "train_loader_WordDef = DataLoader(dataset=train_dataset_WordDef, batch_size=Hyperparams[\"BatchSize\"], shuffle=True, num_workers=0)\n",
    "train_loader_WordExWord = DataLoader(dataset=train_dataset_WordExWord, batch_size=Hyperparams[\"BatchSize\"], shuffle=True, num_workers=0)\n",
    "train_loader_DefExWord = DataLoader(dataset=train_dataset_DefExWord, batch_size=Hyperparams[\"BatchSize\"], shuffle=True, num_workers=0)\n",
    "\n",
    "print(len(WordWord_train_src), len(WordDef_train_src), len(WordExWord_train_src), len(DefExWord_train_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03daeb9-5b6d-4ac2-ba8f-3be77af3fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gluonnlp\n",
    "import copy\n",
    "import random\n",
    "# import gensim.downloader\n",
    "\n",
    "crit_mean = nn.MSELoss()\n",
    "crit_sum = nn.MSELoss(reduction='sum')\n",
    "cos = nn.CosineSimilarity(eps=1e-6)\n",
    "\n",
    "### Data Loading\n",
    "wordsim_sim = []\n",
    "file_object = open(\"./wordsim_data/wordsim353/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    wordsim_sim.append([line[0], line[1], float(line[2])])\n",
    "file_object.close()\n",
    "            \n",
    "wordsim_rel = []\n",
    "file_object = open(\"./wordsim_data/wordsim353/wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    wordsim_rel.append([line[0], line[1], float(line[2])])\n",
    "file_object.close()\n",
    "\n",
    "rw = []\n",
    "file_object = open(\"./wordsim_data/rw/rw.txt\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    rw.append([line[0], line[1], float(line[2])])\n",
    "file_object.close()\n",
    "            \n",
    "men = []\n",
    "file_object = open(\"./wordsim_data/men/MEN/MEN_dataset_natural_form_full\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    men.append([line[0], line[1], float(line[2])])\n",
    "file_object.close()\n",
    "            \n",
    "sem = []\n",
    "file_object1 = open(\"./wordsim_data/SemEval17-Task2/test/subtask1-monolingual/data/en.test.data.txt\", 'r', encoding='utf-8')\n",
    "file_object2 = open(\"./wordsim_data/SemEval17-Task2/test/subtask1-monolingual/keys/en.test.gold.txt\", 'r', encoding='utf-8')\n",
    "for line1, line2 in zip(file_object1, file_object2):\n",
    "    line1 = line1.split()\n",
    "    line2 = line2.split()\n",
    "    sem.append([line1[0], line1[1], float(line2[0])])\n",
    "file_object1.close()\n",
    "file_object2.close()\n",
    "        \n",
    "simlex = []\n",
    "file_object = open(\"./wordsim_data/SimLex-999/SimLex-999.txt\", 'r', encoding='utf-8')\n",
    "file_object.readline() # skip the first line\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    simlex.append([line[0], line[1], float(line[3])])\n",
    "file_object.close()\n",
    "            \n",
    "simverb = []\n",
    "file_object = open(\"./wordsim_data/SimVerb-3000-test.txt\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    simverb.append([line[0], line[1], float(line[3])])\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75cab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils\n",
    "# import gluonnlp\n",
    "import copy\n",
    "import random\n",
    "\n",
    "crit_mean = nn.MSELoss()\n",
    "crit_sum = nn.MSELoss(reduction='sum')\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-5)\n",
    "\n",
    "### Evaluator\n",
    "def Evaluation(model_name, tokenizer, data):\n",
    "    model_score = []; human_score = []\n",
    "    model_name.eval()\n",
    "    for d in data:\n",
    "        w1, w2, score = d\n",
    "#         print(w1, w2, score, end=' ')\n",
    "        w1, w2 = w1.lower().strip(), w2.lower().strip()\n",
    "        w1 = tokenizer(w1, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        w2 = tokenizer(w2, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        w1 = { k: v.to(device) for k, v in w1.items() }\n",
    "        w2 = { k: v.to(device) for k, v in w2.items() }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            w1 = model_name(**w1)[\"last_hidden_state\"][:,0,:]\n",
    "            w2 = model_name(**w2)[\"last_hidden_state\"][:,0,:]\n",
    "        model_score.append(cos(w1,w2).to('cpu').item())\n",
    "        human_score.append(score)\n",
    "    return round(stats.spearmanr(model_score, human_score)[0], 4)\n",
    "\n",
    "def ConcatEvaluation(model_names, data):\n",
    "    model_score = []; human_score = []\n",
    "    for d in data:\n",
    "        w1, w2, score = d\n",
    "        w1 = torch.tensor([Tokenizer.encode(w1)])#.to(device)\n",
    "        w2 = torch.tensor([Tokenizer.encode(w2)])#.to(device)\n",
    "        with torch.no_grad():\n",
    "            w1 = torch.cat((model_names[0](w1)[0][:,0,:], model_names[1](w1)[0][:,0,:]), 1)\n",
    "            w2 = torch.cat((model_names[0](w2)[0][:,0,:], model_names[1](w2)[0][:,0,:]), 1)\n",
    "        model_score.append(cos(w1,w2).to('cpu').item())\n",
    "        human_score.append(score)\n",
    "    return round(stats.spearmanr(model_score, human_score)[0], 4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368d620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15cd7893b84418eb6b584a0361c53bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7564fee7204b492fad0e3d06c4ec99c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1168702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.720\n",
      "WordSim353(Rel) Score 0.483\n",
      "RareWords Score 0.491\n",
      "MEN Score 0.753\n",
      "SEM Score 0.545\n",
      "SimLex Score 0.475\n",
      "SimVerb Score 0.377\n",
      "Average 0.549\n",
      "=====New Best=====\n",
      "save to save/6,7\n",
      "Epoch [1/10], Loss: 94.7987\n",
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b29d963cbd4f63b41294cbc4c30f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1168702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.616\n",
      "WordSim353(Rel) Score 0.419\n",
      "RareWords Score 0.417\n",
      "MEN Score 0.690\n",
      "SEM Score 0.493\n",
      "SimLex Score 0.396\n",
      "SimVerb Score 0.323\n",
      "Average 0.479\n",
      "Epoch [2/10], Loss: 25.9727\n",
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c96a5e34ea49afa94b0504cbd5966e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1168702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.543\n",
      "WordSim353(Rel) Score 0.366\n",
      "RareWords Score 0.360\n",
      "MEN Score 0.641\n",
      "SEM Score 0.455\n",
      "SimLex Score 0.350\n",
      "SimVerb Score 0.249\n",
      "Average 0.423\n",
      "Epoch [3/10], Loss: 25.7403\n",
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad44bda539243c2b519dafc1ce58c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1168702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Training Seq\n",
    "# PAIR_seq = [\"Word-Def\"]\n",
    "# PAIR_seq = [\"Word-Def\",]\n",
    "PAIR_seq = [\"Word-ExamWord\"]\n",
    "# PAIR_seq = [\"Word-Word\", \"Word-ExamWord\", \"Word-Def\", \"Def-ExamWord\"]\n",
    "# PAIR_seq = [\"Def-ExamWord\", \"Word-ExamWord\", \"Word-Def\", \"Word-Word\"]\n",
    "# PAIR_seq = [\"Word-ExamWord\", \"Def-ExamWord\", \"Word-Def\", \"Word-Word\",]\n",
    "# PAIR_seq = [\"Word-Word\", \"Word-Def\", \"Word-ExamWord\"]\n",
    "# PAIR_seq = [\"Word-Word\", \"Word-Def\", \"Word-ExamWord\"]\n",
    "# PAIR_seq = [\"Def-ExamWord\"]\n",
    "\n",
    "#####\n",
    "### Initialization\n",
    "Embedding = AutoModel.from_pretrained('bert-base-uncased')\n",
    "Embedding_ = AutoModel.from_pretrained('bert-base-uncased')\n",
    "# Embedding = AutoModel.from_pretrained('./save/WordWord/3/')\n",
    "# Embedding_ = AutoModel.from_pretrained('./save/WordWord/10/')\n",
    "# Embedding = BertModel.from_pretrained(\"./save/2\")\n",
    "# Embedding_ = BertModel.from_pretrained(\"./save/WordExword/\")\n",
    "# Embedding = nn.DataParallel(Embedding).to(device)\n",
    "# Embedding_ = nn.DataParallel(Embedding_).to(device)\n",
    "#####\n",
    "### Fine-tuning\n",
    "# Hyperparams[\"LearningRate\"] = 1e-4\n",
    "Hyperparams[\"LearningRate\"] = 2e-5\n",
    "# Hyperparams[\"LearningRate\"] = 5e-6\n",
    "# Hyperparams[\"LearningRate\"] = 2e-6\n",
    "Hyperparams[\"NumEpochs\"] = 10\n",
    "#####\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, Embedding.parameters()), lr=Hyperparams[\"LearningRate\"])\n",
    "pbar = tqdm(total=Hyperparams[\"NumEpochs\"])\n",
    "\n",
    "Embedding = nn.DataParallel(Embedding).to(device)\n",
    "# Embedding_ = nn.DataParallel(Embedding).to(device)\n",
    "Embedding_ = copy.deepcopy(Embedding) # Initializer\n",
    "\n",
    "Extractor = torch.tensor([0]).unsqueeze(1).expand(-1,10).to(device)\n",
    "MaxAvg = 0\n",
    "\n",
    "for ep in range(Hyperparams[\"NumEpochs\"]):\n",
    "    random.shuffle(PAIR_list)\n",
    "    Embedding.train()\n",
    "    \n",
    "    Embedding_ = copy.deepcopy(Embedding) # Copier / Epoch\n",
    "    Embedding_.eval()\n",
    "\n",
    "    for PAIR in PAIR_seq:\n",
    "        Embedding_ = copy.deepcopy(Embedding) # PAIR Copier\n",
    "        ### Renew Optimizer\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, Embedding.parameters()), lr=Hyperparams[\"LearningRate\"])\n",
    "        print(PAIR, \"in Training\")\n",
    "        if PAIR == \"Word-Def\":\n",
    "            pbar2 = tqdm(total=len(train_dataset_WordDef), leave=True)\n",
    "            for i, (w, x, y) in enumerate(train_loader_WordDef):\n",
    "                w = { k: v[:,0,:].to(device) for k, v in w.items() }\n",
    "                x = { k: v[:,0,:].to(device) for k, v in x.items() }\n",
    "                y = { k: v[:,0,:].to(device) for k, v in y.items() }\n",
    "                with torch.no_grad(): OutEmb2 = Embedding_(**y)\n",
    "                OutEmb1 = Embedding(**x)\n",
    "                idx = (y[\"input_ids\"] == w[\"input_ids\"][:,0].unsqueeze(1).expand(-1,MAX_SEQ_LEN)).nonzero()\n",
    "                wlen = (w[\"input_ids\"] != Extractor).sum(dim=1)\n",
    "                ### W_t - D_[CLS]\n",
    "                loss = 0\n",
    "#                 for (xx, yy), wl in zip(idx, wlen):\n",
    "#                     loss += crit_sum(torch.mean(OutEmb1[0][xx,yy:yy+wl,:], dim=0), OutEmb2[0][xx,0,:])\n",
    "#                     loss += crit_sum(OutEmb1[0][xx,0,:], torch.mean(OutEmb2[0][xx,yy:yy+wl,:], dim=0))\n",
    "#                 loss /= x[\"input_ids\"].size(0)\n",
    "                ### W_[CLS] - D_[CLS]\n",
    "                loss += crit_sum(OutEmb1[0][:,0,:], OutEmb2[0][:,0,:])\n",
    "                optimizer.zero_grad()\n",
    "                if len(idx): loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar2.update(x[\"input_ids\"].size(0))\n",
    "            pbar2.close()\n",
    "            \n",
    "        elif PAIR == \"Word-Word\": # Need to be Tuned\n",
    "            pbar2 = tqdm(total=len(train_dataset_WordWord), leave=True)\n",
    "            for i, (w, x, y) in enumerate(train_loader_WordWord):\n",
    "                w = { k: v[:,0,:].to(device) for k, v in w.items() }\n",
    "                x = { k: v[:,0,:].to(device) for k, v in x.items() }\n",
    "                y = { k: v[:,0,:].to(device) for k, v in y.items() }\n",
    "                with torch.no_grad(): OutEmb2 = Embedding_(**y)\n",
    "                OutEmb1 = Embedding(**x)\n",
    "                idx = (y[\"input_ids\"] == w[\"input_ids\"][:,0].unsqueeze(1).expand(-1,MAX_SEQ_LEN)).nonzero()\n",
    "                wlen = (w[\"input_ids\"] != Extractor).sum(dim=1)\n",
    "                ###\n",
    "                loss = 0\n",
    "                for (xx, yy), wl in zip(idx, wlen):\n",
    "                    ### W_[CLS] - W_t / W_t - W_[CLS]\n",
    "#                     loss += crit_sum(OutEmb1[0][xx,0,:], torch.mean(OutEmb2[0][xx,yy:yy+wl,:], dim=0)) # Target\n",
    "                    loss += crit_sum(torch.mean(OutEmb1[0][xx,yy:yy+wl,:], dim=0), OutEmb2[0][xx,0,:])\n",
    "                loss /= x[\"input_ids\"].size(0)#*2\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar2.update(x[\"input_ids\"].size(0))\n",
    "            pbar2.close()\n",
    "            \n",
    "        elif PAIR == \"Word-ExamWord\":\n",
    "            pbar2 = tqdm(total=len(train_dataset_WordExWord), leave=True)\n",
    "            for i, (w, x, y) in enumerate(train_loader_WordExWord):\n",
    "                w = { k: v[:,0,:].to(device) for k, v in w.items() }\n",
    "                x = { k: v[:,0,:].to(device) for k, v in x.items() }\n",
    "                y = { k: v[:,0,:].to(device) for k, v in y.items() }\n",
    "                with torch.no_grad(): OutEmb2 = Embedding_(**y)\n",
    "                OutEmb1 = Embedding(**x)\n",
    "                idx = (y[\"input_ids\"] == w[\"input_ids\"][:,0].unsqueeze(1).expand(-1,MAX_SEQ_LEN)).nonzero()\n",
    "                wlen = (w[\"input_ids\"] != Extractor).sum(dim=1)\n",
    "#                 wlen = wlen - SpTokenCnt\n",
    "                loss = 0\n",
    "                for (xx, yy), wl in zip(idx, wlen):\n",
    "                    loss += crit_sum(OutEmb1[0][xx,0,:], torch.mean(OutEmb2[0][xx,yy:yy+wl,:], dim=0))\n",
    "                loss /= x[\"input_ids\"].size(0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar2.update(x[\"input_ids\"].size(0))\n",
    "            pbar2.close()\n",
    "            \n",
    "        elif PAIR == \"Def-ExamWord\":\n",
    "            pbar2 = tqdm(total=len(train_dataset_DefExWord), leave=True)\n",
    "            for i, (w, x, y) in enumerate(train_loader_DefExWord):\n",
    "                w = { k: v[:,0,:].to(device) for k, v in w.items() }\n",
    "                x = { k: v[:,0,:].to(device) for k, v in x.items() }\n",
    "                y = { k: v[:,0,:].to(device) for k, v in y.items() }\n",
    "                ##### Def->ExamWord\n",
    "                with torch.no_grad(): OutEmb2 = Embedding_(**y)\n",
    "                OutEmb1 = Embedding(**x)\n",
    "                idx = (y[\"input_ids\"] == w[\"input_ids\"][:,0].unsqueeze(1).expand(-1,MAX_SEQ_LEN)).nonzero()\n",
    "                wlen = (w[\"input_ids\"] != Extractor).sum(dim=1)\n",
    "                loss = 0\n",
    "                for (xx, yy), wl in zip(idx, wlen):\n",
    "                    loss += crit_mean(OutEmb1[0][xx,0,:], torch.mean(OutEmb2[0][xx,yy:yy+wl,:], dim=0))\n",
    "                loss /= x[\"input_ids\"].size(0)\n",
    "                optimizer.zero_grad()\n",
    "                if len(idx): loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar2.update(x[\"input_ids\"].size(0))\n",
    "            pbar2.close()\n",
    "            \n",
    "                ### ExamWord->Def\n",
    "#                 with torch.no_grad(): OutEmb2 = Embedding_(x)\n",
    "#                 OutEmb1 = Embedding(y)\n",
    "#                 idx = (y == Word[:,0].unsqueeze(1).expand(x.size(0),MAX_SEQ_LEN)).nonzero()\n",
    "#                 loss = 0\n",
    "#                 for xx, yy in idx:\n",
    "#                     loss += crit(OutEmb1[1][xx], OutEmb2[0][xx,yy])\n",
    "#                 optimizer.zero_grad()\n",
    "#                 if len(idx):\n",
    "# #                     loss = loss/len(idx)\n",
    "#                     loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 pbar2.update(Hyperparams[\"BatchSize\"])\n",
    "\n",
    "        else:\n",
    "            print(\"Unknown\")\n",
    "#             loss = crit_mean(OutEmb1[1], OutEmb2[1])\n",
    "\n",
    "        print(\"WordSim Scoring\")\n",
    "        ScoreList = []\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, wordsim_sim))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, wordsim_rel))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, rw))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, men))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, sem))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, simlex))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, simverb))\n",
    "\n",
    "        print('WordSim353(Sim) Score {:.3f}'.format(ScoreList[0]))\n",
    "        print('WordSim353(Rel) Score {:.3f}'.format(ScoreList[1]))\n",
    "        print('RareWords Score {:.3f}'.format(ScoreList[2]))\n",
    "        print('MEN Score {:.3f}'.format(ScoreList[3]))\n",
    "        print('SEM Score {:.3f}'.format(ScoreList[4]))\n",
    "        print('SimLex Score {:.3f}'.format(ScoreList[5]))\n",
    "        print('SimVerb Score {:.3f}'.format(ScoreList[6]))\n",
    "        \n",
    "        Avg = sum(ScoreList)/len(ScoreList)\n",
    "        print(\"Average {:.3f}\".format(Avg))\n",
    "        if Avg > MaxAvg:\n",
    "            MaxAvg = Avg\n",
    "            print(\"=====New Best=====\")\n",
    "#             save_dir = Path('./save/'+GPUIdx)\n",
    "            save_dir = Path('./save/6,7')\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            save_dir = str(save_dir)\n",
    "            print(\"save to\", save_dir)\n",
    "            Embedding.module.save_pretrained(save_dir)\n",
    "            Tokenizer.save_vocabulary(save_dir)\n",
    "\n",
    "    pbar.update(1)\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(ep+1, Hyperparams[\"NumEpochs\"], loss))\n",
    "#     Embedding.module.save_pretrained('./save')\n",
    "    pbar2.close()\n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c84255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W[CLS] - E[TGT]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
