{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63fd3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# import gluonnlp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "GPUIdx = \"0,1\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= GPUIdx\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833ca16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "Tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "Embedding = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29882890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee14a6659e745bcbfd9cdde747b88e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30533 512\n"
     ]
    }
   ],
   "source": [
    "UNCASE = True\n",
    "\n",
    "# FileObject = open(\"../../Data/DefinitionDataset/LexicoDefinition.txt\", 'r', encoding='utf-8')\n",
    "FileObject = open(\"./data/DefinitionDataset.txt\", 'r', encoding='utf-8')\n",
    "VocabDefExamDict = {}\n",
    "MAX_SEQ_LEN = 512\n",
    "pbar = tqdm(total = 1)\n",
    "for line in FileObject:\n",
    "    Contents = line.split(\" (def.) \")\n",
    "    Word = Contents[0].strip()\n",
    "#     print(Word)\n",
    "    if UNCASE: Word = Word.lower()\n",
    "    ### Word -> Definition1 -> [ex1, ex2]\n",
    "    Defs = {}\n",
    "    for c in Contents[1:]:\n",
    "        c = c.split(\" (ex.)\")\n",
    "        Def = c[0].strip(); Examples = c[1].strip()\n",
    "        if UNCASE:\n",
    "            Def = Def.lower()\n",
    "            Examples = Examples.lower()\n",
    "            \n",
    "        Examples = Examples.split(' | ')\n",
    "        ### When don't know MAX_SEQ_LEN\n",
    "#         SEQ_LEN =  max(*[len(Tokenizer.encode(e)) for e in Examples], len(Tokenizer.encode(Def)))\n",
    "#         if SEQ_LEN > MAX_SEQ_LEN:\n",
    "#             MAX_SEQ_LEN = SEQ_LEN\n",
    "        Defs[Def] = Examples if len(Examples) else ''\n",
    "    \n",
    "    if Word in VocabDefExamDict.keys():\n",
    "        VocabDefExamDict[Word].update(Defs)\n",
    "    else:\n",
    "        VocabDefExamDict[Word] = Defs\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "\n",
    "MAX_SEQ_LEN = min(512, MAX_SEQ_LEN)\n",
    "MAX_SEQ_LEN = max(256, MAX_SEQ_LEN)\n",
    "\n",
    "print(len(VocabDefExamDict), MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b817dd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dd129aee994875bce55ab424e17e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4589361f09124064bb1dcf994b1f1391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01be27c741114e31bddf58f16175ea78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b75dc17bee644148ec4a7ecd6692810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PAIR_list = [\"Word-Word\", \"Word-Def\", \"Word-ExamWord\", \"Def-ExamWord\"]\n",
    "PAIR_cnt = dict(zip(PAIR_list, [0]*len(PAIR_list)))\n",
    "\n",
    "WordDef_train_src, DefExam_train_src, WordExam_train_src = [], [], []\n",
    "WordWord_train_src, WordExWord_train_src, DefExWord_train_src = [], [], []\n",
    "WordDef_train_tgt, DefExam_train_tgt, WordExam_train_tgt = [], [], []\n",
    "WordWord_train_tgt, WordExWord_train_tgt, DefExWord_train_tgt = [], [], []\n",
    "\n",
    "### Saved Indices\n",
    "for PAIR in PAIR_list:\n",
    "    pbar = tqdm(total=len(VocabDefExamDict))\n",
    "    for Word in VocabDefExamDict:\n",
    "        if PAIR == \"Word-Word\":\n",
    "            WordWord_train_src.append(Word+'|'+Word)\n",
    "            WordWord_train_tgt.append(Word+'|'+Word)\n",
    "            PAIR_cnt[PAIR] += 1\n",
    "        \n",
    "        elif PAIR == \"Word-Def\":\n",
    "            Defs = list(VocabDefExamDict[Word].keys())\n",
    "            for d in Defs:\n",
    "                WordDef_train_src.append(Word+'|'+Word)\n",
    "                WordDef_train_tgt.append(Word+'|'+d)\n",
    "                PAIR_cnt[PAIR] += 1\n",
    "                \n",
    "        elif PAIR == \"Word-ExamWord\":\n",
    "            w = VocabDefExamDict[Word]\n",
    "            Defs = list(w.keys())\n",
    "            for d in Defs:\n",
    "                for ex in w[d]:\n",
    "                    if ex != '':\n",
    "                        WordExWord_train_src.append(Word+'|'+Word)\n",
    "                        WordExWord_train_tgt.append(Word+'|'+ex)\n",
    "                        PAIR_cnt[PAIR] += 1\n",
    "                        \n",
    "        elif PAIR == \"Def-ExamWord\":\n",
    "            w = VocabDefExamDict[Word]\n",
    "            Defs = list(w.keys())\n",
    "            for d in Defs:\n",
    "                for ex in w[d]:\n",
    "                    if ex != '':\n",
    "                        DefExWord_train_src.append(Word+'|'+d)\n",
    "                        DefExWord_train_tgt.append(Word+'|'+ex)\n",
    "                        PAIR_cnt[PAIR] += 1\n",
    "        else:\n",
    "            print(\"Out-of-Bound\")\n",
    "            break\n",
    "        ### For Debug\n",
    "#         break\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "# WordDef_train_src, DefExam_train_src, WordExam_train_src = torch.tensor(WordDef_train_src), torch.tensor(DefExam_train_src), torch.tensor(WordExam_train_src)\n",
    "# WordWord_train_src, WordExWord_train_src, DefExWord_train_src = torch.tensor(WordWord_train_src), torch.tensor(WordExWord_train_src), torch.tensor(DefExWord_train_src)\n",
    "# WordDef_train_tgt, DefExam_train_tgt, WordExam_train_tgt = torch.tensor(WordDef_train_tgt), torch.tensor(DefExam_train_tgt), torch.tensor(WordExam_train_tgt)\n",
    "# WordWord_train_tgt, WordExWord_train_tgt, DefExWord_train_tgt = torch.tensor(WordWord_train_tgt), torch.tensor(WordExWord_train_tgt), torch.tensor(DefExWord_train_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0111b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DefinitionDataset(Dataset):\n",
    "    def __init__(self, x=None, y=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.len = len(self.x)\n",
    "        self.token_config = {\"truncation\": True, \"padding\": \"max_length\",\n",
    "                            \"max_length\": MAX_SEQ_LEN, \"return_tensors\": \"pt\"}\n",
    "    def __getitem__(self, index):\n",
    "        target_w = self.x[index].split('|')[0]\n",
    "        x = self.x[index].split('|')[1]\n",
    "        y = self.y[index].split('|')[1]\n",
    "        w_tokenized = Tokenizer(target_w, add_special_tokens=False, padding=\"max_length\",\n",
    "                                max_length=10, return_tensors=\"pt\")\n",
    "        x_tokenized = Tokenizer(x, **self.token_config)\n",
    "        y_tokenized = Tokenizer(y, **self.token_config)\n",
    "        return w_tokenized, x_tokenized, y_tokenized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0d02d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30533 93227 1167055 1167055\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "Hyperparams = {\n",
    "    \"NumEpochs\": 10,\n",
    "    \"BatchSize\": 2**7,\n",
    "#     \"BatchSize\": 2**5,\n",
    "#     \"LearningRate\": 2e-3,\n",
    "    \"LearningRate\": 1e-5,\n",
    "}\n",
    "\n",
    "train_dataset_WordWord = DefinitionDataset(x=WordWord_train_src, y=WordWord_train_tgt) if len(WordWord_train_src) else None\n",
    "train_dataset_WordDef = DefinitionDataset(x=WordDef_train_src, y=WordDef_train_tgt) if len(WordDef_train_src) else None\n",
    "train_dataset_WordExWord = DefinitionDataset(x=WordExWord_train_src, y=WordExWord_train_tgt) if len(WordExWord_train_src) else None\n",
    "train_dataset_DefExWord = DefinitionDataset(x=DefExWord_train_src, y=DefExWord_train_tgt)  if len(DefExWord_train_src) else None\n",
    "\n",
    "train_loader_WordWord = DataLoader(dataset=train_dataset_WordWord, batch_size=Hyperparams[\"BatchSize\"], shuffle=True, num_workers=0)\n",
    "train_loader_WordDef = DataLoader(dataset=train_dataset_WordDef, batch_size=Hyperparams[\"BatchSize\"], shuffle=True, num_workers=0)\n",
    "train_loader_WordExWord = DataLoader(dataset=train_dataset_WordExWord, batch_size=Hyperparams[\"BatchSize\"], shuffle=True, num_workers=0)\n",
    "train_loader_DefExWord = DataLoader(dataset=train_dataset_DefExWord, batch_size=Hyperparams[\"BatchSize\"], shuffle=True, num_workers=0)\n",
    "\n",
    "print(len(WordWord_train_src), len(WordDef_train_src), len(WordExWord_train_src), len(DefExWord_train_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03daeb9-5b6d-4ac2-ba8f-3be77af3fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gluonnlp\n",
    "import copy\n",
    "import random\n",
    "# import gensim.downloader\n",
    "\n",
    "crit_mean = nn.MSELoss()\n",
    "crit_sum = nn.MSELoss(reduction='sum')\n",
    "cos = nn.CosineSimilarity(eps=1e-6)\n",
    "\n",
    "### Data Loading\n",
    "wordsim_sim = []\n",
    "file_object = open(\"./wordsim_data/wordsim353/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    wordsim_sim.append([line[0], line[1], float(line[2])])\n",
    "file_object.close()\n",
    "            \n",
    "wordsim_rel = []\n",
    "file_object = open(\"./wordsim_data/wordsim353/wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    wordsim_rel.append([line[0], line[1], float(line[2])])\n",
    "file_object.close()\n",
    "\n",
    "rw = []\n",
    "file_object = open(\"./wordsim_data/rw/rw.txt\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    rw.append([line[0], line[1], float(line[2])])\n",
    "file_object.close()\n",
    "            \n",
    "men = []\n",
    "file_object = open(\"./wordsim_data/men/MEN/MEN_dataset_natural_form_full\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    men.append([line[0], line[1], float(line[2])])\n",
    "file_object.close()\n",
    "            \n",
    "sem = []\n",
    "file_object1 = open(\"./wordsim_data/SemEval17-Task2/test/subtask1-monolingual/data/en.test.data.txt\", 'r', encoding='utf-8')\n",
    "file_object2 = open(\"./wordsim_data/SemEval17-Task2/test/subtask1-monolingual/keys/en.test.gold.txt\", 'r', encoding='utf-8')\n",
    "for line1, line2 in zip(file_object1, file_object2):\n",
    "    line1 = line1.split()\n",
    "    line2 = line2.split()\n",
    "    sem.append([line1[0], line1[1], float(line2[0])])\n",
    "file_object1.close()\n",
    "file_object2.close()\n",
    "        \n",
    "simlex = []\n",
    "file_object = open(\"./wordsim_data/SimLex-999/SimLex-999.txt\", 'r', encoding='utf-8')\n",
    "file_object.readline() # skip the first line\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    simlex.append([line[0], line[1], float(line[3])])\n",
    "file_object.close()\n",
    "            \n",
    "simverb = []\n",
    "file_object = open(\"./wordsim_data/SimVerb-3000-test.txt\", 'r', encoding='utf-8')\n",
    "for line in file_object:\n",
    "    line = line.split()\n",
    "    simverb.append([line[0], line[1], float(line[3])])\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75cab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utils\n",
    "# import gluonnlp\n",
    "import copy\n",
    "import random\n",
    "\n",
    "crit_mean = nn.MSELoss()\n",
    "crit_sum = nn.MSELoss(reduction='sum')\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-5)\n",
    "\n",
    "### Evaluator\n",
    "def Evaluation(model_name, tokenizer, data):\n",
    "    model_score = []; human_score = []\n",
    "    model_name.eval()\n",
    "    for d in data:\n",
    "        w1, w2, score = d\n",
    "#         print(w1, w2, score, end=' ')\n",
    "        w1, w2 = w1.lower().strip(), w2.lower().strip()\n",
    "        w1 = tokenizer(w1, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        w2 = tokenizer(w2, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        w1 = { k: v.to(device) for k, v in w1.items() }\n",
    "        w2 = { k: v.to(device) for k, v in w2.items() }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            w1 = model_name(**w1)[\"last_hidden_state\"][:,0,:]\n",
    "            w2 = model_name(**w2)[\"last_hidden_state\"][:,0,:]\n",
    "        model_score.append(cos(w1,w2).to('cpu').item())\n",
    "        human_score.append(score)\n",
    "    return round(stats.spearmanr(model_score, human_score)[0], 4)\n",
    "\n",
    "def ConcatEvaluation(model_names, data):\n",
    "    model_score = []; human_score = []\n",
    "    for d in data:\n",
    "        w1, w2, score = d\n",
    "        w1 = torch.tensor([Tokenizer.encode(w1)])#.to(device)\n",
    "        w2 = torch.tensor([Tokenizer.encode(w2)])#.to(device)\n",
    "        with torch.no_grad():\n",
    "            w1 = torch.cat((model_names[0](w1)[0][:,0,:], model_names[1](w1)[0][:,0,:]), 1)\n",
    "            w2 = torch.cat((model_names[0](w2)[0][:,0,:], model_names[1](w2)[0][:,0,:]), 1)\n",
    "        model_score.append(cos(w1,w2).to('cpu').item())\n",
    "        human_score.append(score)\n",
    "    return round(stats.spearmanr(model_score, human_score)[0], 4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368d620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaadd87913184997ba70fe045a40e0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4a0302b2e84e178ab27f27e098bdf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.633\n",
      "WordSim353(Rel) Score 0.422\n",
      "RareWords Score 0.447\n",
      "MEN Score 0.728\n",
      "SEM Score 0.500\n",
      "SimLex Score 0.486\n",
      "SimVerb Score 0.308\n",
      "Average 0.503\n",
      "=====New Best=====\n",
      "save to save/0,1\n",
      "Word-Def in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334b589c54e840029ff6bf2d75b7e380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.521\n",
      "WordSim353(Rel) Score 0.296\n",
      "RareWords Score 0.405\n",
      "MEN Score 0.647\n",
      "SEM Score 0.417\n",
      "SimLex Score 0.465\n",
      "SimVerb Score 0.337\n",
      "Average 0.441\n",
      "Def-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33ab0b7514842b6bb8561e467fdb233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.590\n",
      "WordSim353(Rel) Score 0.380\n",
      "RareWords Score 0.467\n",
      "MEN Score 0.718\n",
      "SEM Score 0.492\n",
      "SimLex Score 0.485\n",
      "SimVerb Score 0.425\n",
      "Average 0.508\n",
      "=====New Best=====\n",
      "save to save/0,1\n",
      "Word-Word in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9899d18f54945d2b948f8890b62266c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.645\n",
      "WordSim353(Rel) Score 0.446\n",
      "RareWords Score 0.484\n",
      "MEN Score 0.759\n",
      "SEM Score 0.546\n",
      "SimLex Score 0.511\n",
      "SimVerb Score 0.428\n",
      "Average 0.546\n",
      "=====New Best=====\n",
      "save to save/0,1\n",
      "Epoch [1/10], Loss: 7.0405\n",
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c373692c65a40379992af4055028945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.586\n",
      "WordSim353(Rel) Score 0.369\n",
      "RareWords Score 0.414\n",
      "MEN Score 0.712\n",
      "SEM Score 0.491\n",
      "SimLex Score 0.435\n",
      "SimVerb Score 0.341\n",
      "Average 0.478\n",
      "Word-Def in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628e9af945ba492fb833995896bf8f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.554\n",
      "WordSim353(Rel) Score 0.308\n",
      "RareWords Score 0.400\n",
      "MEN Score 0.625\n",
      "SEM Score 0.410\n",
      "SimLex Score 0.425\n",
      "SimVerb Score 0.361\n",
      "Average 0.441\n",
      "Def-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c106ed10c5472f9fb932ba3d468ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.559\n",
      "WordSim353(Rel) Score 0.341\n",
      "RareWords Score 0.432\n",
      "MEN Score 0.675\n",
      "SEM Score 0.449\n",
      "SimLex Score 0.418\n",
      "SimVerb Score 0.375\n",
      "Average 0.464\n",
      "Word-Word in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c14252583794cf0bae2c9acd1dac91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.650\n",
      "WordSim353(Rel) Score 0.434\n",
      "RareWords Score 0.406\n",
      "MEN Score 0.728\n",
      "SEM Score 0.527\n",
      "SimLex Score 0.481\n",
      "SimVerb Score 0.366\n",
      "Average 0.513\n",
      "Epoch [2/10], Loss: 8.3201\n",
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820dabca555143a79a5cc64486b4695b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.597\n",
      "WordSim353(Rel) Score 0.398\n",
      "RareWords Score 0.420\n",
      "MEN Score 0.710\n",
      "SEM Score 0.488\n",
      "SimLex Score 0.439\n",
      "SimVerb Score 0.348\n",
      "Average 0.486\n",
      "Word-Def in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13919d307de7459c9dfdd024dfcf7397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353(Sim) Score 0.557\n",
      "WordSim353(Rel) Score 0.324\n",
      "RareWords Score 0.401\n",
      "MEN Score 0.621\n",
      "SEM Score 0.412\n",
      "SimLex Score 0.415\n",
      "SimVerb Score 0.356\n",
      "Average 0.441\n",
      "Def-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18fb31d6af8403c8fb808c4df0e6f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353(Sim) Score 0.576\n",
      "WordSim353(Rel) Score 0.354\n",
      "RareWords Score 0.422\n",
      "MEN Score 0.671\n",
      "SEM Score 0.448\n",
      "SimLex Score 0.412\n",
      "SimVerb Score 0.358\n",
      "Average 0.463\n",
      "Word-Word in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82675fef0d9d41908c29e07b384750d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.652\n",
      "WordSim353(Rel) Score 0.436\n",
      "RareWords Score 0.401\n",
      "MEN Score 0.719\n",
      "SEM Score 0.518\n",
      "SimLex Score 0.473\n",
      "SimVerb Score 0.368\n",
      "Average 0.509\n",
      "Epoch [3/10], Loss: 8.3090\n",
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92521c8c0144f2da56d16e2c44b072a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.605\n",
      "WordSim353(Rel) Score 0.391\n",
      "RareWords Score 0.420\n",
      "MEN Score 0.702\n",
      "SEM Score 0.488\n",
      "SimLex Score 0.441\n",
      "SimVerb Score 0.349\n",
      "Average 0.485\n",
      "Word-Def in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab0b94c1389479f9fc3284f59d3252e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.571\n",
      "WordSim353(Rel) Score 0.333\n",
      "RareWords Score 0.402\n",
      "MEN Score 0.628\n",
      "SEM Score 0.430\n",
      "SimLex Score 0.415\n",
      "SimVerb Score 0.351\n",
      "Average 0.447\n",
      "Def-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aebbe9887404698951e904302438a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.573\n",
      "WordSim353(Rel) Score 0.359\n",
      "RareWords Score 0.410\n",
      "MEN Score 0.665\n",
      "SEM Score 0.453\n",
      "SimLex Score 0.403\n",
      "SimVerb Score 0.338\n",
      "Average 0.457\n",
      "Word-Word in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a3014fdb3644ceb0fdfc49242a16ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353(Sim) Score 0.645\n",
      "WordSim353(Rel) Score 0.434\n",
      "RareWords Score 0.401\n",
      "MEN Score 0.710\n",
      "SEM Score 0.518\n",
      "SimLex Score 0.468\n",
      "SimVerb Score 0.367\n",
      "Average 0.506\n",
      "Epoch [4/10], Loss: 9.2678\n",
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdf68ef056642baa3cae34d39a8b5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353(Sim) Score 0.608\n",
      "WordSim353(Rel) Score 0.402\n",
      "RareWords Score 0.421\n",
      "MEN Score 0.704\n",
      "SEM Score 0.486\n",
      "SimLex Score 0.441\n",
      "SimVerb Score 0.350\n",
      "Average 0.487\n",
      "Word-Def in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca15e6aafaf458dbb0ecf4d662f6b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/93227 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.580\n",
      "WordSim353(Rel) Score 0.352\n",
      "RareWords Score 0.404\n",
      "MEN Score 0.645\n",
      "SEM Score 0.444\n",
      "SimLex Score 0.420\n",
      "SimVerb Score 0.350\n",
      "Average 0.456\n",
      "Def-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ccea85bab3429697c71725c3f86985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim Scoring\n",
      "WordSim353(Sim) Score 0.584\n",
      "WordSim353(Rel) Score 0.377\n",
      "RareWords Score 0.401\n",
      "MEN Score 0.675\n",
      "SEM Score 0.460\n",
      "SimLex Score 0.406\n",
      "SimVerb Score 0.325\n",
      "Average 0.461\n",
      "Word-Word in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607cdbb999d142c489f3b8c27c243d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30533 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordSim353(Sim) Score 0.648\n",
      "WordSim353(Rel) Score 0.432\n",
      "RareWords Score 0.395\n",
      "MEN Score 0.705\n",
      "SEM Score 0.512\n",
      "SimLex Score 0.464\n",
      "SimVerb Score 0.362\n",
      "Average 0.503\n",
      "Epoch [5/10], Loss: 9.1807\n",
      "Word-ExamWord in Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a86e63b85d467f8ade307ff70d9db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1167055 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Training Seq\n",
    "PAIR_seq = [\"Word-ExamWord\", \"Word-Def\", \"Def-ExamWord\", \"Word-Word\"]\n",
    "\n",
    "#####\n",
    "### Initialization\n",
    "Embedding = AutoModel.from_pretrained('bert-base-uncased')\n",
    "Embedding_ = AutoModel.from_pretrained('bert-base-uncased')\n",
    "# Embedding = AutoModel.from_pretrained('./save/WordWord/3/')\n",
    "# Embedding_ = AutoModel.from_pretrained('./save/WordWord/10/')\n",
    "# Embedding = BertModel.from_pretrained(\"./save/2\")\n",
    "# Embedding_ = BertModel.from_pretrained(\"./save/WordExword/\")\n",
    "# Embedding = nn.DataParallel(Embedding).to(device)\n",
    "# Embedding_ = nn.DataParallel(Embedding_).to(device)\n",
    "#####\n",
    "### Fine-tuning\n",
    "# Hyperparams[\"LearningRate\"] = 1e-4\n",
    "# Hyperparams[\"LearningRate\"] = 2e-5\n",
    "# Hyperparams[\"LearningRate\"] = 5e-6\n",
    "Hyperparams[\"LearningRate\"] = 1e-6\n",
    "Hyperparams[\"NumEpochs\"] = 10\n",
    "#####\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, Embedding.parameters()), lr=Hyperparams[\"LearningRate\"])\n",
    "pbar = tqdm(total=Hyperparams[\"NumEpochs\"])\n",
    "\n",
    "Embedding = nn.DataParallel(Embedding).to(device)\n",
    "# Embedding_ = nn.DataParallel(Embedding).to(device)\n",
    "Embedding_ = copy.deepcopy(Embedding) # Initializer\n",
    "Embedding_.eval()\n",
    "Extractor = torch.tensor([0]).unsqueeze(1).expand(-1,10).to(device)\n",
    "MaxAvg = 0\n",
    "optimizer_ww = torch.optim.Adam(filter(lambda p: p.requires_grad, Embedding.parameters()), lr=Hyperparams[\"LearningRate\"])\n",
    "optimizer_wd = torch.optim.Adam(filter(lambda p: p.requires_grad, Embedding.parameters()), lr=Hyperparams[\"LearningRate\"])\n",
    "optimizer_we = torch.optim.Adam(filter(lambda p: p.requires_grad, Embedding.parameters()), lr=Hyperparams[\"LearningRate\"])\n",
    "optimizer_de = torch.optim.Adam(filter(lambda p: p.requires_grad, Embedding.parameters()), lr=Hyperparams[\"LearningRate\"])\n",
    "\n",
    "for ep in range(Hyperparams[\"NumEpochs\"]):\n",
    "#     random.shuffle(PAIR_list)\n",
    "    Embedding.train()\n",
    "    \n",
    "#     Embedding_ = copy.deepcopy(Embedding) # Copier / Epoch\n",
    "#     Embedding_.eval()\n",
    "\n",
    "    for PAIR in PAIR_seq:\n",
    "#         Embedding_ = copy.deepcopy(Embedding) # PAIR Copier\n",
    "#         Embedding_.eval()\n",
    "        ### Renew Optimizer\n",
    "        print(PAIR, \"in Training\")\n",
    "        if PAIR == \"Word-Def\":\n",
    "            pbar2 = tqdm(total=len(train_dataset_WordDef), leave=True)\n",
    "            for i, (w, x, y) in enumerate(train_loader_WordDef):\n",
    "                w = { k: v[:,0,:].to(device) for k, v in w.items() }\n",
    "                x = { k: v[:,0,:].to(device) for k, v in x.items() }\n",
    "                y = { k: v[:,0,:].to(device) for k, v in y.items() }\n",
    "                with torch.no_grad(): OutEmb2 = Embedding_(**x)\n",
    "                OutEmb1 = Embedding(**y)\n",
    "                idx = (x[\"input_ids\"] == w[\"input_ids\"][:,0].unsqueeze(1).expand(-1,MAX_SEQ_LEN)).nonzero()\n",
    "                wlen = (w[\"input_ids\"] != Extractor).sum(dim=1)\n",
    "                ### W_t - D_[CLS]\n",
    "                loss = 0\n",
    "                for (xx, yy), wl in zip(idx, wlen):\n",
    "                    break\n",
    "#                     loss += crit_sum(torch.mean(OutEmb1[0][xx,yy:yy+wl,:], dim=0), OutEmb2[0][xx,0,:])\n",
    "#                     loss += crit_sum(OutEmb1[0][xx,0,:], torch.mean(OutEmb2[0][xx,yy:yy+wl,:], dim=0))\n",
    "                ### W_[CLS] - D_[CLS]\n",
    "                loss += crit_sum(OutEmb1[0][:,0,:], OutEmb2[0][:,0,:])\n",
    "                loss /= x[\"input_ids\"].size(0)\n",
    "                optimizer_wd.zero_grad()\n",
    "                if len(idx): loss.backward()\n",
    "                optimizer_wd.step()\n",
    "                pbar2.update(x[\"input_ids\"].size(0))\n",
    "            pbar2.close()\n",
    "            \n",
    "        elif PAIR == \"Word-Word\": # Need to be Tuned\n",
    "            pbar2 = tqdm(total=len(train_dataset_WordWord), leave=True)\n",
    "            for i, (w, x, y) in enumerate(train_loader_WordWord):\n",
    "                w = { k: v[:,0,:].to(device) for k, v in w.items() }\n",
    "                x = { k: v[:,0,:].to(device) for k, v in x.items() }\n",
    "                y = { k: v[:,0,:].to(device) for k, v in y.items() }\n",
    "                with torch.no_grad(): OutEmb2 = Embedding_(**y)\n",
    "                OutEmb1 = Embedding(**x)\n",
    "                idx = (y[\"input_ids\"] == w[\"input_ids\"][:,0].unsqueeze(1).expand(-1,MAX_SEQ_LEN)).nonzero()\n",
    "                wlen = (w[\"input_ids\"] != Extractor).sum(dim=1)\n",
    "                ###\n",
    "                loss = 0\n",
    "                for (xx, yy), wl in zip(idx, wlen):\n",
    "                    ### W_[CLS] - W_t / W_t - W_[CLS]\n",
    "                    loss += crit_sum(OutEmb1[0][xx,0,:], torch.mean(OutEmb2[0][xx,1:1+wl,:], dim=0)) # Target\n",
    "#                     loss += crit_sum(torch.mean(OutEmb1[0][xx,yy:yy+wl,:], dim=0), OutEmb2[0][xx,0,:])\n",
    "                loss /= x[\"input_ids\"].size(0)#*2\n",
    "                optimizer_ww.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_ww.step()\n",
    "                pbar2.update(x[\"input_ids\"].size(0))\n",
    "            pbar2.close()\n",
    "            \n",
    "        elif PAIR == \"Word-ExamWord\":\n",
    "            pbar2 = tqdm(total=len(train_dataset_WordExWord), leave=True)\n",
    "            for i, (w, x, y) in enumerate(train_loader_WordExWord):\n",
    "                w = { k: v[:,0,:].to(device) for k, v in w.items() }\n",
    "                x = { k: v[:,0,:].to(device) for k, v in x.items() }\n",
    "                y = { k: v[:,0,:].to(device) for k, v in y.items() }\n",
    "                with torch.no_grad(): OutEmb2 = Embedding_(**y)\n",
    "                OutEmb1 = Embedding(**x)\n",
    "                idx = (y[\"input_ids\"] == w[\"input_ids\"][:,0].unsqueeze(1).expand(-1,MAX_SEQ_LEN)).nonzero()\n",
    "                wlen = (w[\"input_ids\"] != Extractor).sum(dim=1)\n",
    "#                 wlen = wlen - SpTokenCnt\n",
    "                loss = 0\n",
    "                for (xx, yy), wl in zip(idx, wlen):\n",
    "                    loss += crit_sum(OutEmb1[0][xx,0,:], torch.mean(OutEmb2[0][xx,yy:yy+wl,:], dim=0))\n",
    "                loss /= x[\"input_ids\"].size(0)\n",
    "                optimizer_we.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_we.step()\n",
    "                pbar2.update(x[\"input_ids\"].size(0))\n",
    "            pbar2.close()\n",
    "            \n",
    "        elif PAIR == \"Def-ExamWord\":\n",
    "            pbar2 = tqdm(total=len(train_dataset_DefExWord), leave=True)\n",
    "            for i, (w, x, y) in enumerate(train_loader_DefExWord):\n",
    "                w = { k: v[:,0,:].to(device) for k, v in w.items() }\n",
    "                x = { k: v[:,0,:].to(device) for k, v in x.items() }\n",
    "                y = { k: v[:,0,:].to(device) for k, v in y.items() }\n",
    "                ##### Def->ExamWord\n",
    "                with torch.no_grad(): OutEmb2 = Embedding_(**y)\n",
    "                OutEmb1 = Embedding(**x)\n",
    "                idx = (y[\"input_ids\"] == w[\"input_ids\"][:,0].unsqueeze(1).expand(-1,MAX_SEQ_LEN)).nonzero()\n",
    "                wlen = (w[\"input_ids\"] != Extractor).sum(dim=1)\n",
    "                loss = 0\n",
    "                for (xx, yy), wl in zip(idx, wlen):\n",
    "                    loss += crit_sum(OutEmb1[0][xx,0,:], torch.mean(OutEmb2[0][xx,yy:yy+wl,:], dim=0))\n",
    "                loss /= x[\"input_ids\"].size(0)\n",
    "                optimizer_de.zero_grad()\n",
    "                if len(idx): loss.backward()\n",
    "                optimizer_de.step()\n",
    "                pbar2.update(x[\"input_ids\"].size(0))\n",
    "            pbar2.close()\n",
    "            \n",
    "                ### ExamWord->Def\n",
    "#                 with torch.no_grad(): OutEmb2 = Embedding_(x)\n",
    "#                 OutEmb1 = Embedding(y)\n",
    "#                 idx = (y == Word[:,0].unsqueeze(1).expand(x.size(0),MAX_SEQ_LEN)).nonzero()\n",
    "#                 loss = 0\n",
    "#                 for xx, yy in idx:\n",
    "#                     loss += crit(OutEmb1[1][xx], OutEmb2[0][xx,yy])\n",
    "#                 optimizer.zero_grad()\n",
    "#                 if len(idx):\n",
    "# #                     loss = loss/len(idx)\n",
    "#                     loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 pbar2.update(Hyperparams[\"BatchSize\"])\n",
    "\n",
    "        else:\n",
    "            print(\"Unknown\")\n",
    "#             loss = crit_mean(OutEmb1[1], OutEmb2[1])\n",
    "\n",
    "        print(\"WordSim Scoring\")\n",
    "        ScoreList = []\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, wordsim_sim))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, wordsim_rel))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, rw))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, men))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, sem))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, simlex))\n",
    "        ScoreList.append(Evaluation(Embedding, Tokenizer, simverb))\n",
    "\n",
    "        print('WordSim353(Sim) Score {:.3f}'.format(ScoreList[0]))\n",
    "        print('WordSim353(Rel) Score {:.3f}'.format(ScoreList[1]))\n",
    "        print('RareWords Score {:.3f}'.format(ScoreList[2]))\n",
    "        print('MEN Score {:.3f}'.format(ScoreList[3]))\n",
    "        print('SEM Score {:.3f}'.format(ScoreList[4]))\n",
    "        print('SimLex Score {:.3f}'.format(ScoreList[5]))\n",
    "        print('SimVerb Score {:.3f}'.format(ScoreList[6]))\n",
    "        \n",
    "        Avg = sum(ScoreList)/len(ScoreList)\n",
    "        print(\"Average {:.3f}\".format(Avg))\n",
    "        if Avg > MaxAvg:\n",
    "            MaxAvg = Avg\n",
    "            print(\"=====New Best=====\")\n",
    "            save_dir = Path('./save/'+GPUIdx)\n",
    "#             save_dir = Path('./save/10,11')\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            save_dir = str(save_dir)\n",
    "            print(\"save to\", save_dir)\n",
    "            Embedding.module.save_pretrained(save_dir)\n",
    "            Tokenizer.save_vocabulary(save_dir)\n",
    "            \n",
    "            Embedding_ = copy.deepcopy(Embedding) # PAIR Copier\n",
    "            Embedding_.eval()\n",
    "\n",
    "    pbar.update(1)\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(ep+1, Hyperparams[\"NumEpochs\"], loss))\n",
    "#     Embedding.module.save_pretrained('./save')\n",
    "    pbar2.close()\n",
    "    \n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88abad5d-fccf-4a65-ad62-ca045add1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAIR_seq = [\"Word-Word\", \"Word-Def\", \"Def-ExamWord\", \"Word-ExamWord\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
